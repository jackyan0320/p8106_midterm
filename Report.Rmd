---
title: "Predicting Country-Level Life Expectancy in 2013"
author: "Jianghui Lin jl5172, Zixu Wang zw2541, Jack Yan xy2395"
date: "4/5/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')
library(caret)
library(tidyverse)
library(mgcv)
library(modelr)
library(corrplot)
library(earth)
library(vip)
library(pdp)
library(patchwork)
```

```{r, include=FALSE}
# Import data
set.seed(123)
data = read.csv("./data/life_expectancy.csv") %>% as.tibble %>% 
  janitor::clean_names() %>% 
  filter(year == 2013) %>% 
  na.omit() %>% 
  select(-country, -year) 

```

## Introduction

Although there have been a lot of studies undertaken in the past on factors affecting life expectancy considering demographic variables, income composition, and mortality rates, it was found that effect of immunization and human development index was not taken into account in the past. Hence, this gives the motivation to resolve both the factors stated previously by formulating a regression model based on linear regression and non-linear regression. Important immunization like Hepatitis B, Polio and Diphtheria will also be considered. 			
       
This dataset includes the immunization factors, mortality factors, economic factors, social factors and other health-related factors from 193 countries in 2013. Since the observations this dataset is based on different countries, we can use the analysis results to determine which predicting factors contribute to lower life expectancy. Also, we use non-linear models to try to capture the relationship between predictors and life expectancy. This information would be helpful for government and related agencies to serve their residents better and thus improve their life expectancy. 

We decided to focus on data collected in 2013 and omit any observation with NA values. After cleaning, we have 130 valid observations. Data are centered and scaled in the `caret` package when fitting the models.

## Exploratory Data Analysis

```{r, include=FALSE, eval = F}
x <- model.matrix(life_expectancy~., data)[,-1]
y <- data$life_expectancy
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5) 
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1) 
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2) 
trellis.par.set(theme1)
data_rename = 
  data %>% 
  rename(income = income_composition_of_resources,
         per_expend = percentage_expenditure,
         to_expend = total_expenditure,
         death5 = under_five_deaths,
         thin5_9 = thinness_5_9_years,
         thin1_19 = thinness_1_19_years)
x_rename <- model.matrix(life_expectancy~., data_rename)[,-1]

par(cex = 0.7)
corrplot(cor(x_rename), tl.srt = 45, order = 'hclust', type = 'upper', title = "Correlation between Variables")
```

According to the correlation plot above, we have found several interesting correlations between different variables. For example, Hepatitis B (HepB) immunization coverage has strong positive correlation with Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage, Gross Domestic Product(GDP) has a strong positive correlation with Expenditure on health(per_expand) and BMI has a strong positive correlation with income, etc. We have determined to take care of these correlations in later analysis.

We haven’t found any near zero variables in our dataset. Therefore, we decided to continue further analysis on all variables.

## Models

Both non-linear models (GAM, KNN and MARS) and linear models (Ridge, lasso, and PCR) were built on all the data to analyse the relationship between life expenctancy and the predictors. 10-fold cross-validation was conducted to find the optimal tuning parameter(s) in KNN, MARS, Ridge, the lasso, and PCR. Since we only have 19 variables, we did not do any variable selection prior to model-building, and let the model decide which variables should be included. The only exception is for GAM, where the variables were selected manually based on GCV.

### Non-linear Models

#### _GAM_

GAM was used to model the non-linear relation between predictors and response, which might be closer to the truth than those given by linear models. With our small sample size (n = 130) and large number of predictors (p = 19), it is impossible to use `caret::train` to do automatic variable selection, because the model would have more coefficients than data. Instead, we manually selected 8 most important variables from all the 19 variables according to GCV score. The plot shows that `income_composition_of_resources`, `adult_mortality`, and `total_expenditure` are the most influential non-linear predictors.

```{r, echo=FALSE, eval = FALSE}
fit.gam3 = gam(data = data, 
              life_expectancy ~ 
                s(adult_mortality) + s(bmi) + s(total_expenditure) + s(hiv_aids) +
                s(thinness_1_19_years) + s(schooling) + s(income_composition_of_resources) + s(percentage_expenditure)
              )

par(mfrow = c(3, 3))
plot(fit.gam3)
```

#### _KNN_
```{r, warning = FALSE, echo = FALSE, eval=F}
set.seed(123123)
ctrl2 <- trainControl(method = "cv", number = 10)
knn.fit <- train(x, y,
                 method = "knn",
                 tuneGrid = data.frame(k=c(1:length(y))),
                 trControl = ctrl2)
plot(knn.fit)
knn.fit$bestTune
```

All the 19 predictors were included in the KNN model. The optimal tuning parameter k = 6 was chosen with minimum RMSE by 10-fold cross-validation. The importance of variables was obtained by `VarImp` function. Using all the data, the top 5 most important variables are `income_composition_of_resources`, `schooling`, `hiv_aids`, `adult_mortality`, and `bmi`. 

#### _MARS_
```{r, echo = FALSE, eval = FALSE}
mars_grid <- expand.grid(degree = 1:5, 
                         nprune = 2:20)
ctrl1 = trainControl(method = "cv", number = 10)
set.seed(123123)
mars.fit <- train(x, y,
                 method = "earth",
                 tuneGrid = mars_grid,
                 trControl = ctrl1)
# tunning parameter selection for MARS
#ggplot(mars.fit)

# Importance plot for predictors included in the final model
# p1 <- vip(mars.fit, num_features = 8, bar = FALSE, value = "gcv") + ggtitle("GCV")
# p2 <- vip(mars.fit, num_features = 8, bar = FALSE, value = "rss") + ggtitle("RSS")
# gridExtra::grid.arrange(p1, p2, ncol = 2)

# partial dependence plots (PDPs) for `income_composition_of_resources` and `adult_mortality`
p3 <- pdp::partial(mars.fit, pred.var = "income_composition_of_resources", grid.resolution = 10) %>% autoplot()
p4 <- pdp::partial(mars.fit, pred.var = "adult_mortality", grid.resolution = 10) %>% autoplot()
gridExtra::grid.arrange(p3, p4,  ncol = 3)
```

Using 10-fold cross-validation, we found the optimal model with 6 retained terms and 1 degree of interaction. Among the 6 retained variables, `income_composition_of_resources` and `adult_mortality` contribute most to the reduction of residual sums of squares (RSS) and GCV score.

### Linear Models

We created four types of linear models here and the assumptions are shown below:

* Linearity: The variables and response actually exhibit a linear relationship.

* Independency: Observations should be independent.

* Homoscedasticity: For each value of the predictor (x), the variance of the response(Y) should be the same.

* Normality: For each value of the predictor (x), the distribution of the response(Y) is normal. The errors should be normally distributed.

#### _Linear model using least squares_

We included all predictors for least square. The least square model is relatively simple and applicable which based on maximum-likelihood solution. The best linear unbiased estimators can be obtained if Gauss-Markov conditions applied. By checking the p-value (test of whether each coefficient equals to zero) of each predictor, we found first significant predictors are `hiv_aids` and `income_composition_of_resources`, second significant predictor is `total_expenditure`. 

#### _Ridge_

All predictor variables were included. The tuning parameters were picked from a grid of $\lambda$ value from -1 to 1 with length=100. The 10-fold cross-validation was performed in order to pick the value of tuning parameter—the $\lambda$ with the smallest error. Using the varlmp() function, we found that `income_composition_of_resources`, `status Developing`, `hiv_aids`, `schooling`, `total_expenditure` are more significant than other variables. 

#### _Lasso_

All predictor variables were included. The 10-fold cross-validation was performed in order to pick the value of tuning parameter—the $\lambda$ with the smallest error. Only 9 of 19 variables were involved in this model, and the variables like `income_composition_of_resources`, `hiv_aids` and `total_expenditure` are considered more significant than the other predictors.

#### _PCR_

All predictor variables were included. The 10-fold cross-validation was performed in order to pick the value of tuning parameter with the smallest error. The significant variables in PCR include `Income_composition_of_resources`, `schooling`, `hiv_aids`, `adult_mortality`.

```{r, include=FALSE}
# ridge regression model
x_all = model.matrix(life_expectancy~., data)[,-1]
y_all = data$life_expectancy

ctrl1 = trainControl(method = "cv", number = 10)

ridge.fit = train(x = x_all, 
                  y = y_all,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 0, 
                                         lambda = exp(seq(-1, 1, length=100))),
                  trControl = ctrl1)

# lasso regression model
lasso.fit = train(x = x_all, 
                  y = y_all,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 1, 
                                         lambda = exp(seq(-5, 3, length=100))),
                  trControl = ctrl1,
                  preProc = c("center", "scale"))
varImp(lasso.fit)
# PCR model
pcr.fit = train(x = x_all, 
                y = y_all,
                method = "pcr",
                tuneLength = length(data),
                trControl = ctrl1,
                scale = TRUE)
```

```{r, fig.width=10, echo=FALSE, eval = FALSE}
p_ridge = ggplot(ridge.fit, xTrans = function(x) log(x)) +
          labs(title = "Tuning Parameters in RIDGE Model") 
p_lasso = ggplot(lasso.fit, xTrans = function(x) log(x)) +
          labs(title = "Tuning Parameters in LASSO Model")
p_pcr = ggplot(pcr.fit, highlight = TRUE) + theme_bw() + 
        labs(title = "Tuning parameters in PCR Model")

p_ridge + p_lasso + p_pcr
```

## Model Comparison By Cross-Validation

To compare model predictive performance, 10-fold cross-validation was performed. Tuning parameters were decided by 10-fold cross-validation using the training data set. RMSE was obtained using the testing data set.

```{r, eval=FALSE, echo=FALSE}
set.seed(123123)
cv_df = 
  crossv_kfold(data, 10) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))

mars_grid <- expand.grid(degree = 1:2, 
                         nprune = 2:20)

ctrl1 = trainControl(method = "cv", number = 10)
ctrl2 <- trainControl(method = "cv", number = 20)

# GAM
cv_df = 
  cv_df %>% 
  mutate(gam_mod = map(train, ~gam(data = .x, 
              life_expectancy ~ 
                s(adult_mortality) + s(bmi) + s(total_expenditure) + s(hiv_aids) + s(total_expenditure) +
                s(thinness_1_19_years) + s(schooling) + s(income_composition_of_resources) + s(percentage_expenditure))),
         rmse_gam = map2_dbl(gam_mod, test, ~rmse(model = .x, data = .y))
         ) 

# MARS
cv_df = 
  cv_df %>% 
  mutate(mars_mod = map(train, ~train(data = .x,
                 life_expectancy~.,
                 method = "earth",
                 tuneGrid = mars_grid,
                 trControl = ctrl1)),
         rmse_mars = map2_dbl(mars_mod, test, ~rmse(model = .x, data = .y))
         ) 
# KNN
cv_df = 
  cv_df %>% 
  mutate(knn_mod = map(train, 
                ~train(data = .x,
                 life_expectancy~.,
                 method = "knn",
                 tuneLength = 100,
                 trControl = ctrl2)),
         knn_tune = map(knn_mod, ~.$bestTune),
         rmse_knn = map2_dbl(knn_mod, test, ~rmse(model = .x, data = .y))
         ) 

# LM
cv_df = 
  cv_df %>% 
  mutate(lm_mod = map(train, ~lm(life_expectancy~., data = .x)),
         rmse_lm = map2_dbl(lm_mod, test, ~rmse(model = .x, data = .y))) 
# Ridge
cv_df = 
  cv_df %>% 
  mutate(ridge_mod = map(train, ~train(data = .x, 
                  life_expectancy~.,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 0, lambda = exp(seq(-1, 1, length=100))),
                  trControl = ctrl1)),
         rmse_ridge = map2_dbl(ridge_mod, test, ~rmse(model = .x, data = .y)))

# The lasso
cv_df = 
  cv_df %>% 
  mutate(lasso_mod = map(train, ~train(data = .x, 
                  life_expectancy~.,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 1, lambda = exp(seq(-5, 1, length=100))),
                  trControl = ctrl1,
                  preProc = c("center", "scale"))),
         rmse_lasso = map2_dbl(lasso_mod, test, ~rmse(model = .x, data = .y)))

# PCR
cv_df = 
  cv_df %>% 
  mutate(pcr_mod = map(train, ~train(data = .x, 
                  life_expectancy~.,
                  method = "pcr",
                  tuneGrid = data.frame(ncomp = seq(1:18)),
                  trControl = ctrl1,
                  scale = TRUE)),
         rmse_pcr = map2_dbl(pcr_mod, test, ~rmse(model = .x, data = .y)))

cv_df = 
  cv_df %>% 
  mutate(trrmse_gam = map2_dbl(gam_mod, train, ~rmse(model = .x, data = .y)),
         trrmse_mars = map2_dbl(mars_mod, train, ~rmse(model = .x, data = .y)),
         trrmse_knn = map2_dbl(knn_mod, train, ~rmse(model = .x, data = .y)),
         trrmse_lm = map2_dbl(lm_mod, train, ~rmse(model = .x, data = .y)),
         trrmse_ridge = map2_dbl(ridge_mod, train, ~rmse(model = .x, data = .y)),
         trrmse_lasso = map2_dbl(lasso_mod, train, ~rmse(model = .x, data = .y)),         
         trrmse_pcr = map2_dbl(pcr_mod, train, ~rmse(model = .x, data = .y)))

test_rmse = 
  cv_df %>%  
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model),
         type = 'test') 

tr_rmse = 
  cv_df %>%  
  select(starts_with("trrmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "trrmse_", ""),
         model = fct_inorder(model),
         type = 'train') 

cv_result = 
  bind_rows(test_rmse, tr_rmse) %>%
  mutate(type = as_factor(type),
         type = fct_relevel(type, 'train', 'test'), 
         model = fct_inorder(model))

saveRDS(cv_result, 'cv_result.rds')
```

```{r, echo=FALSE, eval = FALSE}
cv_result = readRDS('cv_result.rds')
cv_result %>% 
  ggplot(aes(x = model, y = rmse, color=type)) + 
    geom_boxplot() +
    labs(title = "Training and Testing RMSE")
```

The above plot shows the distribution of RMSE in the 10 folds. Among the nonlinear models, GAM fits the training data most closely, but MARS has the overall best prediction performance. The KNN model has both the highest training and the highest testing RMSE, showing lack of fit and predictability. The 4 linear models have similar performance, among which lasso has the lowest mean RMSE. Overall, MARS generates the lowest RMSE, so it has the best performance for our data. It is worth noting, however, that the range of RMSE for MARS model is wider than the linear models. Therefore, linear models may have a more stable performace than non-linear models for our data.

## Discussion 
**Interpretation and Importance of some predictors**

Among the four types of linear model, Lasso regression model has the best performance on the testing data. By checking the coefficient and importance of different variables, we found there exist a extremly strong positive relationship between `income_composition_of_resources` and `life_expectancy`. The increasing in income may highly enlongate the life expectancy.

**Limitations: **
The data used to fit the models were a subset of longitudinal data for 193 countries during 2000-2015. For lack of knowledge in longitudinal data analysis, we only used the 2013 data. Although high correlation within each country was avoided and thus the independent assumption is valid, we might lose considerable useful information in the other years. 

After omitting `NAs`, we only had 130 observations to fit models. Small number of observations is especially problematic for cross-validated variable selection in GAM, because the number of coefficients exceeds number of observations. To bypass this issue, we manually selected a small subset of variables into GAM. Using GAM, we could better understand the non-linear relationship between life expectancy and the predictor. However, this underestimates the test RMSE of GAM in the model comparision step, because we inevitably used prior information in the testing set to select variables.


# Plots and tables 

```{r corrplot, echo=FALSE, fig.align='center'}
x <- model.matrix(life_expectancy~., data)[,-1]
y <- data$life_expectancy
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5) 
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1) 
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2) 
trellis.par.set(theme1)
data_rename = 
  data %>% 
  rename(income = income_composition_of_resources,
         per_expend = percentage_expenditure,
         to_expend = total_expenditure,
         death5 = under_five_deaths,
         thin5_9 = thinness_5_9_years,
         thin1_19 = thinness_1_19_years)
x_rename <- model.matrix(life_expectancy~., data_rename)[,-1]

par(cex = 0.7)
corrplot(cor(x_rename), tl.srt = 45, order = 'hclust', type = 'upper')
```

\center __Figure 1__ Correlation among predictors \center

```{r gam_fit_plot, echo=FALSE, fig.align='center'}
fit.gam3 = gam(data = data, 
              life_expectancy ~ 
                s(adult_mortality) + s(bmi) + s(total_expenditure) + s(hiv_aids) +
                s(thinness_1_19_years) + s(schooling) + s(income_composition_of_resources) + s(percentage_expenditure)
              )

par(mfrow = c(3, 3))
plot(fit.gam3)
```

\center __Figure 2__ Non-linear Relationship between predictors and life expectancy in GAM \center

```{r mars_tuning, echo = FALSE, fig.align='center'}
mars_grid <- expand.grid(degree = 1:5, 
                         nprune = 2:20)
ctrl1 = trainControl(method = "cv", number = 10)
set.seed(123123)
mars.fit <- train(x, y,
                 method = "earth",
                 tuneGrid = mars_grid,
                 trControl = ctrl1)
# tunning parameter selection for MARS
#ggplot(mars.fit)

# Importance plot for predictors included in the final model
# p1 <- vip(mars.fit, num_features = 8, bar = FALSE, value = "gcv") + ggtitle("GCV")
# p2 <- vip(mars.fit, num_features = 8, bar = FALSE, value = "rss") + ggtitle("RSS")
# gridExtra::grid.arrange(p1, p2, ncol = 2)

# partial dependence plots (PDPs) for `income_composition_of_resources` and `adult_mortality`
p3 <- pdp::partial(mars.fit, pred.var = "income_composition_of_resources", grid.resolution = 10) %>% autoplot()
p4 <- pdp::partial(mars.fit, pred.var = "adult_mortality", grid.resolution = 10) %>% autoplot()
gridExtra::grid.arrange(p3, p4,  ncol = 3)
```

\center __Figure 3__ Non-linear Relationship between 2 major predictors and life expectancy in MARS \center

```{r nonlinear_tuning, warning = FALSE, echo=FALSE, fig.align='center'}
set.seed(123123)
ctrl2 <- trainControl(method = "cv", number = 10)
knn.fit <- train(x, y,
                 method = "knn",
                 tuneGrid = data.frame(k=c(1:length(y))),
                 trControl = ctrl2)
ggplot(mars.fit) + ggplot(knn.fit) + theme_bw()
```

\center __Figure 4__ Tuning parameter selection for non-linear models  \center

```{r linear_tuning, fig.width=10, echo=FALSE, fig.align='center'}
p_ridge = ggplot(ridge.fit, xTrans = function(x) log(x)) +
          labs(title = "Tuning Parameters in RIDGE Model") 
p_lasso = ggplot(lasso.fit, xTrans = function(x) log(x)) +
          labs(title = "Tuning Parameters in LASSO Model")
p_pcr = ggplot(pcr.fit, highlight = TRUE) + theme_bw() + 
        labs(title = "Tuning parameters in PCR Model")

p_ridge + p_lasso + p_pcr
```

\center __Figure 4__ Tuning parameter selection for linear models  \center

```{r cv_model_compare, echo=FALSE, fig.align='center'}
cv_result = readRDS('cv_result.rds')
cv_result %>% 
  ggplot(aes(x = model, y = rmse, color=type)) + 
    geom_boxplot() +
    theme_bw()
```

\center **Figure 5** Model comparision based on RMSE  \center
